# Lab Assignment 1: Text Preprocessing and Regular Expressions
# •	Implement tokenization, stemming, and lemmatization using NLTK and spaCy.
# •	Use regular expressions for tasks such as extracting email addresses, phone numbers, and hashtags from a given text dataset of minimum 5 pages.

# Step 1: Install Required Libraries
!pip install nltk spacy
!python -m spacy download en_core_web_sm

# Step 2: Import Libraries
import nltk
import re
import spacy
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer


# Download necessary NLTK data
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
# Load SpaCy model
nlp = spacy.load("en_core_web_sm")


# Step 3: Simulate a sample 5-page dataset (you can load your own .txt or .csv here)
text_data = """
Contact me at john.doe@example.com or jane_doe22@sample.org.
My phone number is +1-800-555-1234 or (212) 555-4567.
I love #MachineLearning and #AI!
Barack Obama was the 44th president of the United States.
SpaCy is great for NLP. NLTK is also useful.

Email me at test.email@gmail.com or hello@mydomain.org.
Call me at 987-654-3210 or 1234567890.
Follow #Python and #DataScience on Twitter.
The cat sat on the mat. The cats are sitting on the mats.
"""

# Preprocess into lines like different pages for simulation
pages = text_data.strip().split('\n')
print("=== Tokenization ===")
for i, page in enumerate(pages):
    tokens = word_tokenize(page)
    print(f"\nPage {i+1} Tokens:\n", tokens)


stemmer = PorterStemmer()

print("\n=== Stemming ===")
for i, page in enumerate(pages):
    tokens = word_tokenize(page)
    stemmed = [stemmer.stem(word) for word in tokens]
    print(f"\nPage {i+1} Stemmed:\n", stemmed)


print("\n=== Lemmatization (spaCy) ===")
for i, page in enumerate(pages):
    doc = nlp(page)
    lemmatized = [token.lemma_ for token in doc]
    print(f"\nPage {i+1} Lemmatized:\n", lemmatized)


email_pattern = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+'
phone_pattern = r'(\+?\d{1,3})?[\s\-\.]?\(?\d{2,4}\)?[\s\-\.]?\d{3,4}[\s\-\.]?\d{4}'
hashtag_pattern = r'#\w+'

print("\n=== Regex Extraction ===")
for i, page in enumerate(pages):
    emails = re.findall(email_pattern, page)
    phones = re.findall(phone_pattern, page)
    hashtags = re.findall(hashtag_pattern, page)

    print(f"\nPage {i+1} Results:")
    print("Emails:", emails)
    print("Phone Numbers:", phones)
    print("Hashtags:", hashtags)

--------------------------------------------------------------------------------------------------------------


Simulate Dataset:
# Step 3: Simulate a sample 5-page dataset (you can load your own .txt or .csv here)
text_data = """
Contact me at john.doe@example.com or jane_doe22@sample.org.
My phone number is +1-800-555-1234 or (212) 555-4567.
I love #MachineLearning and #AI!
Barack Obama was the 44th president of the United States.
SpaCy is great for NLP. NLTK is also useful.

Email me at test.email@gmail.com or hello@mydomain.org.
Call me at 987-654-3210 or 1234567890.
Follow #Python and #DataScience on Twitter.
The cat sat on the mat. The cats are sitting on the mats.
"""

# Preprocess into lines like different pages for simulation
pages = text_data.strip().split('\n')
text_data = """...""": Defines a multi-line string containing the sample text that will be analyzed. This text includes examples of emails, phone numbers, hashtags, names, and sentences suitable for tokenization, stemming, and lemmatization.
pages = text_data.strip().split('\n'): This line preprocesses the text_data.
.strip(): Removes any leading or trailing whitespace from the entire block of text.
.split('\n'): Splits the cleaned text block into a list of strings, using the newline character (\n) as the delimiter. This effectively treats each line of the sample text as a separate "page" for the purpose of iterating through the data.

3. Text Preprocessing:

This section demonstrates three core preprocessing steps applied page by page.

Tokenization (NLTK):
print("=== Tokenization ===")
for i, page in enumerate(pages):
    tokens = word_tokenize(page)
    print(f"\nPage {i+1} Tokens:\n", tokens)
Purpose: To break down the text into individual words or units (tokens).
for i, page in enumerate(pages):: Loops through the list of pages, getting both the index (i, starting from 0) and the content (page). i+1 is used for display to show page numbers starting from 1.
tokens = word_tokenize(page): Uses NLTK's word_tokenize function to split the current page string into a list of tokens. This function is generally good at separating punctuation from words.
print(...): Prints the original page number and the resulting list of tokens for that page.

Stemming (NLTK):
stemmer = PorterStemmer() # Initialize the Porter Stemmer

print("\n=== Stemming ===")
for i, page in enumerate(pages):
    tokens = word_tokenize(page) # Tokenize the page first
    stemmed = [stemmer.stem(word) for word in tokens] # Apply stemming to each token
    print(f"\nPage {i+1} Stemmed:\n", stemmed)
Purpose: To reduce words to their root or base form, often by removing suffixes. Stemming is a cruder process than lemmatization; the resulting stem may not be a valid word (e.g., "running" -> "run", "cats" -> "cat", "sitting" -> "sit").
stemmer = PorterStemmer(): Creates an instance of the PorterStemmer, a common stemming algorithm.
for i, page in enumerate(pages):: Loops through pages again.
tokens = word_tokenize(page): Tokenizes the page, as stemming works on individual words.
stemmed = [stemmer.stem(word) for word in tokens]: Uses a list comprehension to iterate through the tokens and apply the stem() method of the stemmer to each word. This returns the stemmed version of the word.
print(...): Prints the stemmed tokens for each page.

Lemmatization (spaCy):
print("\n=== Lemmatization (spaCy) ===")
for i, page in enumerate(pages):
    doc = nlp(page) # Process the page using the spaCy model
    lemmatized = [token.lemma_ for token in doc] # Extract the lemma for each token from the processed Doc object
    print(f"\nPage {i+1} Lemmatized:\n", lemmatized)
Purpose: To reduce words to their dictionary or base form (the lemma), considering the word's meaning and part of speech. The result is usually a valid word (e.g., "running" -> "run", "better" -> "good", "cats" -> "cat"). Lemmatization is generally more sophisticated than stemming.
for i, page in enumerate(pages):: Loops through pages.
doc = nlp(page): This is the key spaCy processing step. The nlp object processes the entire page text through its pipeline, which includes tokenization, POS tagging, dependency parsing, and lemmatization. The result is a Doc object containing structured information about the text.
lemmatized = [token.lemma_ for token in doc]: Iterates through the token objects that are part of the doc. Each token object in spaCy has a .lemma_ attribute which stores the lemma determined by the spaCy model during processing. A list comprehension collects these lemmas.
print(...): Prints the lemmatized tokens for each page.

4. Regular Expression Extraction:
# Define regex patterns for extraction
email_pattern = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+' # Pattern for common email formats
phone_pattern = r'(\+?\d{1,3})?[\s\-\.]?\(?\d{2,4}\)?[\s\-\.]?\d{3,4}[\s\-\.]?\d{4}' # Pattern for various phone number formats
hashtag_pattern = r'#\w+' # Pattern for hashtags (starting with # followed by word characters)

print("\n=== Regex Extraction ===")
for i, page in enumerate(pages):
    emails = re.findall(email_pattern, page) # Find all matches for the email pattern
    phones = re.findall(phone_pattern, page) # Find all matches for the phone pattern
    hashtags = re.findall(hashtag_pattern, page) # Find all matches for the hashtag pattern

    print(f"\nPage {i+1} Results:")
    print("Emails:", emails)
    print("Phone Numbers:", phones)
    print("Hashtags:", hashtags)
Purpose: To find and extract specific sequences of characters that match predefined patterns (emails, phone numbers, hashtags) using regular expressions.
email_pattern = r'...': Defines a raw string (r'...') containing the regular expression pattern for email addresses. This pattern looks for one or more alphanumeric characters or _.+-, followed by @, followed by one or more alphanumeric characters or -, followed by a literal dot (\.), followed by one or more alphanumeric characters or -..
phone_pattern = r'...': Defines a raw string with a regular expression pattern for phone numbers. This is a more complex pattern designed to match various common formats, including optional country codes (+1), optional separators (space, hyphen, dot [\s\-\.]), optional parentheses \(?\), and sequences of digits (\d+).
hashtag_pattern = r'#\w+': Defines a raw string with the pattern for hashtags. It looks for a literal hash symbol (\#) followed by one or more "word characters" (\w+), which typically includes letters, numbers, and the underscore.
for i, page in enumerate(pages):: Loops through each page.
emails = re.findall(email_pattern, page): Uses the re.findall() function to find all non-overlapping occurrences of the email_pattern within the current page string. It returns a list of all matched strings.
phones = re.findall(phone_pattern, page): Finds all occurrences of the phone_pattern in the current page.
hashtags = re.findall(hashtag_pattern, page): Finds all occurrences of the hashtag_pattern in the current page.
print(...): Prints the page number and the lists of extracted emails, phone numbers, and hashtags found on that page.